{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from sktime.datasets import load_from_arff_to_dataframe\n",
    "from pyts.utils import windowed_view\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data, apply_fft=True):\n",
    "\n",
    "    def aux_extract(data):\n",
    "        features = []\n",
    "        features.append(data.mean(1))\n",
    "        features.append(data.std(1))\n",
    "        features.append(np.abs(data - data.mean(1)[:, np.newaxis, :]).mean(1))\n",
    "        features.append(data.min(1))\n",
    "        features.append(data.max(1))\n",
    "        features.append(data.max(1) - data.min(1))\n",
    "\n",
    "        features.append(np.median(data, axis=1))\n",
    "        features.append(np.median(np.abs(data - np.median(data,1)[:, np.newaxis, :]),1))\n",
    "        features.append(np.subtract(*np.percentile(data,[75, 25],1)))\n",
    "        features.append(np.count_nonzero(data < 0, 1))\n",
    "        features.append(np.count_nonzero(data >= 0, 1))\n",
    "        features.append(np.count_nonzero(data > data.mean(1)[:, np.newaxis, :], 1))\n",
    "\n",
    "        features.append(np.apply_along_axis(lambda x: len(find_peaks(x)[0]), 1, data))\n",
    "        features.append(np.nan_to_num(skew(data,axis=1)))\n",
    "        features.append(np.nan_to_num(kurtosis(data, axis=1)))\n",
    "        features.append(np.mean(data**2,axis=1))\n",
    "        features.append(np.mean(np.sqrt(np.sum(data**2,axis=-1)),axis=1)[:, np.newaxis])\n",
    "        features.append(np.sum(np.mean(np.abs(data),axis=1),axis=1)[:, np.newaxis])\n",
    "\n",
    "        features = np.concatenate(features,axis=1)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    seq_len = data.shape[1]\n",
    "    fft_data = np.abs(np.fft.fft(data,axis=1))[:, 1:(seq_len//2+1), :]\n",
    "\n",
    "    result = aux_extract(data)\n",
    "    if apply_fft:\n",
    "        result = np.concatenate([result, aux_extract(fft_data)],axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_window(data, window_size=10, window_step=3, apply_fft=True):\n",
    "\n",
    "    data = np.stack(\n",
    "        [\n",
    "            windowed_view(data[:,:,idx], window_size, window_step)\n",
    "            for idx in range(data.shape[-1])\n",
    "        ],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    def aux_extract(data, prefix=\"\"):\n",
    "        n_samples, n_windows, _, dims = data.shape\n",
    "\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        features.append(data.mean(2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}mean{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "    \n",
    "        features.append(data.std(2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}std{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.abs(data - data.mean(2)[:, :, np.newaxis, :]).mean(2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}mae{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "    \n",
    "        features.append(data.min(2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}min{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(data.max(2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}max{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(data.max(2).reshape(n_samples,-1) - data.min(2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}extremadiff{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.median(data, axis=2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}median{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.median(np.abs(data - np.median(data,2)[:, :, np.newaxis, :]),2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}mad{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.subtract(*np.percentile(data,[75, 25],2)).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}iqr{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.count_nonzero(data < 0, 2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}neg{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.count_nonzero(data >= 0, 2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}pos{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.count_nonzero(data > data.mean(2)[:, :, np.newaxis, :], 2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}above_mean{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.apply_along_axis(lambda x: len(find_peaks(x)[0]), 2, data).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}peaks{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.nan_to_num(skew(data,axis=2).reshape(n_samples,-1)))\n",
    "        feature_names.extend([f\"{prefix}skew{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.nan_to_num(kurtosis(data, axis=2).reshape(n_samples,-1)))\n",
    "        feature_names.extend([f\"{prefix}krt{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.mean(data**2,axis=2).reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}energy{i}_dim{j}\" for i in range(n_windows) for j in range(dims)])\n",
    "\n",
    "        features.append(np.mean(np.sqrt(np.sum(data**2,axis=-1)),axis=2)[:, :, np.newaxis].reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}acc{i}\" for i in range(n_windows)])\n",
    "\n",
    "        features.append(np.sum(np.mean(np.abs(data),axis=2),axis=2)[:, :, np.newaxis].reshape(n_samples,-1))\n",
    "        feature_names.extend([f\"{prefix}sma{i}\" for i in range(n_windows)])\n",
    "\n",
    "        features = np.concatenate(features,axis=1)\n",
    "\n",
    "        return features, feature_names\n",
    "    \n",
    "    seq_len = data.shape[2]\n",
    "    fft_data = np.abs(np.fft.fft(data, axis=2))[:, :, 1:(seq_len//2+1), :]\n",
    "    result,res_names = aux_extract(data)\n",
    "    if apply_fft:\n",
    "        fft_res, fft_feat = aux_extract(fft_data, \"fft_\")\n",
    "        result = np.concatenate([result, fft_res],axis=1)\n",
    "        res_names = res_names + fft_feat\n",
    "    return result, res_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data + convert numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"mitbih\" # [\"racketsports\", \"mitbih\", \"ptbdb\"]\n",
    "DATA_PATH = \"data\"\n",
    "LABEL_COL = 187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2numpy(X):\n",
    "    N = len(X)\n",
    "    S = len(X.iloc[0][0])\n",
    "    H = len(X.columns)\n",
    "    return np.stack(X.values.reshape(-1)).reshape(N,S,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10712/4162814476.py:55: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  features.append(np.nan_to_num(skew(data,axis=2).reshape(n_samples,-1)))\n",
      "/tmp/ipykernel_10712/4162814476.py:58: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  features.append(np.nan_to_num(kurtosis(data, axis=2).reshape(n_samples,-1)))\n",
      "/tmp/ipykernel_10712/4162814476.py:55: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  features.append(np.nan_to_num(skew(data,axis=2).reshape(n_samples,-1)))\n",
      "/tmp/ipykernel_10712/4162814476.py:58: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  features.append(np.nan_to_num(kurtosis(data, axis=2).reshape(n_samples,-1)))\n"
     ]
    }
   ],
   "source": [
    "if DATASET == \"racketsports\":\n",
    "\n",
    "    X_train, y_train = load_from_arff_to_dataframe(\n",
    "        os.path.join(DATA_PATH, \"RacketSports/RacketSports_TRAIN.arff\")\n",
    "    )\n",
    "\n",
    "    X_test, y_test = load_from_arff_to_dataframe(\n",
    "        os.path.join(DATA_PATH, \"RacketSports/RacketSports_TEST.arff\")\n",
    "    )\n",
    "\n",
    "    rs_train = dataframe2numpy(X_train)\n",
    "    rs_test = dataframe2numpy(X_test)\n",
    "\n",
    "    label2id = {el:i for i, el in enumerate(list(np.unique(y_train)))}\n",
    "\n",
    "    target_train = pd.Series(y_train).apply(lambda x:label2id[x]).values\n",
    "    target_test = pd.Series(y_test).apply(lambda x:label2id[x]).values\n",
    "\n",
    "    train_features = feature_extraction_window(rs_train)\n",
    "    test_features = feature_extraction_window(rs_test)\n",
    "\n",
    "elif DATASET == \"mitbih\":\n",
    "\n",
    "    mit_bih_train = pd.read_csv(os.path.join(\"data\",\"ECG\",\"mitbih_train.csv\"),header=None)\n",
    "    target_train = mit_bih_train[LABEL_COL].copy().values\n",
    "    mit_bih_train.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    mit_bih_test = pd.read_csv(os.path.join(\"data\",\"ECG\",\"mitbih_test.csv\"),header=None)\n",
    "    target_test = mit_bih_test[LABEL_COL].copy().values\n",
    "    mit_bih_test.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    mitbih_train = mit_bih_train.values\n",
    "    mitbih_test = mit_bih_test.values\n",
    "\n",
    "    train_features,feature_names = feature_extraction_window(mitbih_train[:,:,np.newaxis],20,8)\n",
    "    test_features,_ = feature_extraction_window(mitbih_test[:,:,np.newaxis],20,8)\n",
    "\n",
    "elif DATASET == \"ptbdb\":\n",
    "    \n",
    "    abnormal = pd.read_csv(os.path.join(\"data\",\"ECG\",\"ptbdb_abnormal.csv\"),header=None)\n",
    "    normal = pd.read_csv(os.path.join(\"data\",\"ECG\",\"ptbdb_normal.csv\"),header=None)\n",
    "\n",
    "    train_abn, test_abn = train_test_split(abnormal, test_size=0.2, random_state=42)\n",
    "    train_nor, test_nor = train_test_split(normal, test_size=0.2, random_state=42)\n",
    "\n",
    "    ptbdb_train = pd.concat([train_abn, train_nor]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    ptbdb_test = pd.concat([test_abn, test_nor]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    target_train = ptbdb_train[LABEL_COL].copy()\n",
    "    ptbdb_train.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    target_test = ptbdb_test[LABEL_COL].copy()\n",
    "    ptbdb_test.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    ptbdb_train = ptbdb_train.values\n",
    "    ptbdb_test = ptbdb_test.values\n",
    "\n",
    "    train_features = feature_extraction_window(ptbdb_train[:,:,np.newaxis],20,8)\n",
    "    test_features = feature_extraction_window(ptbdb_test[:,:,np.newaxis],20,8)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection & standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecter = VarianceThreshold(2)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecter = selecter.fit(train_features)\n",
    "selected_train = selecter.transform(train_features)\n",
    "selected_test = selecter.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = scaler.fit(selected_train)\n",
    "scaled_train = scaler.transform(selected_train)\n",
    "scaled_test = scaler.transform(selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87554, 74)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = selecter.get_support(indices=True)\n",
    "selected_feats = [elem for i,elem in enumerate(feature_names) if i in idxs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"svm\" # [\"svm\", \"rforest\", \"xgb\"]\n",
    "MODEL = None\n",
    "MODEL_PARAMS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"svm\":\n",
    "    MODEL = SVC()\n",
    "    MODEL_PARAMS = {\n",
    "        \"kernel\":[\"linear\",\"poly\",\"rbf\"],\n",
    "        \"C\":[1.0,5.0,10.0,20.0]\n",
    "    }\n",
    "\n",
    "elif MODEL_NAME == \"rforest\":\n",
    "    MODEL = RandomForestClassifier()\n",
    "    MODEL_PARAMS = {\n",
    "        \"n_estimators\":range(40,301,20),\n",
    "        \"max_depth\":[3, 5, 8, 12],\n",
    "        \"max_samples\":[0.4, 0.7, 1.0]\n",
    "    }\n",
    "\n",
    "elif MODEL_NAME == \"xgb\":\n",
    "    MODEL = XGBClassifier()\n",
    "    MODEL_PARAMS = {\n",
    "        \"n_estimators\":range(40,101,20),\n",
    "        \"max_depth\":[3, 5, 8],\n",
    "        \"learning_rate\":[1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "\n",
    "else:\n",
    "    print(\"Untested model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(MODEL, MODEL_PARAMS,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8305/1576037832.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    820\u001b[0m                     )\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    823\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "clf = clf.fit(scaled_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12, 'max_samples': 1.0, 'n_estimators': 240}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "results_df = []\n",
    "\n",
    "for i, params in enumerate(pd.DataFrame(clf.cv_results_)[\"params\"]):\n",
    "    if MODEL_NAME == \"svm\":\n",
    "        curr_model = SVC(**params)\n",
    "    elif MODEL_NAME == \"rforest\":\n",
    "        curr_model = RandomForestClassifier(**params)\n",
    "    elif MODEL_NAME == \"xgb\":\n",
    "        curr_model = XGBClassifier(**params)\n",
    "    \n",
    "    curr_model = curr_model.fit(scaled_train, target_train)\n",
    "\n",
    "    y_true = target_test\n",
    "    y_pred = curr_model.predict(scaled_test)\n",
    "\n",
    "\n",
    "    param_str = \", \".join([f\"{k}={v}\" for k,v in params.items()])\n",
    "    row = [param_str]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    row.append(accuracy)\n",
    "    res = list(map(np.mean, precision_recall_fscore_support(y_true, y_pred, zero_division=0)))\n",
    "    res2 = list(map(np.std, precision_recall_fscore_support(y_true, y_pred, zero_division=0)))\n",
    "    \n",
    "    mean_std = zip(res[:-1], res2[:-1])\n",
    "    for mean_std_tuple in mean_std:\n",
    "        row.extend(list(mean_std_tuple))\n",
    "\n",
    "    results_df.append(row)\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"Hyperparameters\",\n",
    "    \"Accuracy\",\n",
    "    \"Mean precision\",\n",
    "    \"Std precision\",\n",
    "    \"Mean recall\",\n",
    "    \"Std recall\",\n",
    "    \"Mean f1\",\n",
    "    \"Std f1\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(results_df,columns=columns).round(3)\n",
    "df.to_excel(f\"{DATASET}_{MODEL_NAME}_results.xlsx\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97     18118\n",
      "         1.0       0.99      0.49      0.65       556\n",
      "         2.0       0.96      0.72      0.82      1448\n",
      "         3.0       0.91      0.31      0.46       162\n",
      "         4.0       1.00      0.88      0.94      1608\n",
      "\n",
      "    accuracy                           0.95     21892\n",
      "   macro avg       0.96      0.68      0.77     21892\n",
      "weighted avg       0.95      0.95      0.95     21892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = classification_report(target_test, clf.predict(scaled_test))\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
