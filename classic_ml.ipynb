{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from sktime.datasets import load_from_arff_to_dataframe\n",
    "from pyts.utils import windowed_view\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data, apply_fft=True):\n",
    "\n",
    "    def aux_extract(data):\n",
    "        features = []\n",
    "        features.append(data.mean(1))\n",
    "        features.append(data.std(1))\n",
    "        features.append(np.abs(data - data.mean(1)[:, np.newaxis, :]).mean(1))\n",
    "        features.append(data.min(1))\n",
    "        features.append(data.max(1))\n",
    "        features.append(data.max(1) - data.min(1))\n",
    "\n",
    "        features.append(np.median(data, axis=1))\n",
    "        features.append(np.median(np.abs(data - np.median(data,1)[:, np.newaxis, :]),1))\n",
    "        features.append(np.subtract(*np.percentile(data,[75, 25],1)))\n",
    "        features.append(np.count_nonzero(data < 0, 1))\n",
    "        features.append(np.count_nonzero(data >= 0, 1))\n",
    "        features.append(np.count_nonzero(data > data.mean(1)[:, np.newaxis, :], 1))\n",
    "\n",
    "        features.append(np.apply_along_axis(lambda x: len(find_peaks(x)[0]), 1, data))\n",
    "        features.append(skew(data,axis=1))\n",
    "        features.append(kurtosis(data, axis=1))\n",
    "        features.append(np.mean(data**2,axis=1))\n",
    "        features.append(np.mean(np.sqrt(np.sum(data**2,axis=-1)),axis=1)[:, np.newaxis])\n",
    "        features.append(np.sum(np.mean(np.abs(data),axis=1),axis=1)[:, np.newaxis])\n",
    "\n",
    "        features = np.concatenate(features,axis=1)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    seq_len = data.shape[1]\n",
    "    fft_data = np.abs(np.fft.fft(data,axis=1))[:, 1:(seq_len//2+1), :]\n",
    "\n",
    "    result = aux_extract(data)\n",
    "    if apply_fft:\n",
    "        result = np.concatenate([result, aux_extract(fft_data)],axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_window(data, window_size=10, window_step=3, apply_fft=True):\n",
    "\n",
    "    data = np.stack(\n",
    "        [\n",
    "            windowed_view(data[:,:,idx], window_size, window_step)\n",
    "            for idx in range(data.shape[-1])\n",
    "        ],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    def aux_extract(data):\n",
    "        n_samples = data.shape[0]\n",
    "\n",
    "        features = []\n",
    "\n",
    "        features.append(data.mean(2).reshape(n_samples,-1))\n",
    "        features.append(data.std(2).reshape(n_samples,-1))\n",
    "        features.append(np.abs(data - data.mean(2)[:, :, np.newaxis, :]).mean(2).reshape(n_samples,-1))\n",
    "        features.append(data.min(2).reshape(n_samples,-1))\n",
    "        features.append(data.max(2).reshape(n_samples,-1))\n",
    "        features.append(data.max(2).reshape(n_samples,-1) - data.min(2).reshape(n_samples,-1))\n",
    "\n",
    "        features.append(np.median(data, axis=2).reshape(n_samples,-1))\n",
    "        features.append(np.median(np.abs(data - np.median(data,2)[:, :, np.newaxis, :]),2).reshape(n_samples,-1))\n",
    "        features.append(np.subtract(*np.percentile(data,[75, 25],2)).reshape(n_samples,-1))\n",
    "        features.append(np.count_nonzero(data < 0, 2).reshape(n_samples,-1))\n",
    "        features.append(np.count_nonzero(data >= 0, 2).reshape(n_samples,-1))\n",
    "        features.append(np.count_nonzero(data > data.mean(2)[:, :, np.newaxis, :], 2).reshape(n_samples,-1))\n",
    "\n",
    "        features.append(np.apply_along_axis(lambda x: len(find_peaks(x)[0]), 2, data).reshape(n_samples,-1))\n",
    "        # features.append(skew(data,axis=2).reshape(n_samples,-1))\n",
    "        # features.append(kurtosis(data, axis=2).reshape(n_samples,-1))\n",
    "        features.append(np.mean(data**2,axis=2).reshape(n_samples,-1))\n",
    "        features.append(np.mean(np.sqrt(np.sum(data**2,axis=-1)),axis=2)[:, :, np.newaxis].reshape(n_samples,-1))\n",
    "        features.append(np.sum(np.mean(np.abs(data),axis=2),axis=2)[:, :, np.newaxis].reshape(n_samples,-1))\n",
    "\n",
    "        features = np.concatenate(features,axis=1)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    seq_len = data.shape[2]\n",
    "    fft_data = np.abs(np.fft.fft(data, axis=2))[:, :, 1:(seq_len//2+1), :]\n",
    "    result = aux_extract(data)\n",
    "    if apply_fft:\n",
    "        result = np.concatenate([result, aux_extract(fft_data)],axis=1)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data + convert numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = [\"RacketSports\", \"MITBIH\", \"PTBDB\"]\n",
    "DATA_PATH = \"data\"\n",
    "LABEL_COL = 187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2numpy(X):\n",
    "    N = len(X)\n",
    "    S = len(X.iloc[0][0])\n",
    "    H = len(X.columns)\n",
    "    return np.stack(X.values.reshape(-1)).reshape(N,S,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"RacketSports\":\n",
    "\n",
    "    X_train, y_train = load_from_arff_to_dataframe(\n",
    "        os.path.join(DATA_PATH, \"RacketSports/RacketSports_TRAIN.arff\")\n",
    "    )\n",
    "\n",
    "    X_test, y_test = load_from_arff_to_dataframe(\n",
    "        os.path.join(DATA_PATH, \"RacketSports/RacketSports_TEST.arff\")\n",
    "    )\n",
    "\n",
    "    rs_train = dataframe2numpy(X_train)\n",
    "    rs_test = dataframe2numpy(X_test)\n",
    "\n",
    "    label2id = {el:i for i, el in enumerate(list(np.unique(y_train)))}\n",
    "\n",
    "    target_train = pd.Series(y_train).apply(lambda x:label2id[x]).values\n",
    "    target_test = pd.Series(y_test).apply(lambda x:label2id[x]).values\n",
    "\n",
    "    train_features = feature_extraction_window(rs_train)\n",
    "    test_features = feature_extraction_window(rs_test)\n",
    "\n",
    "elif DATASET == \"MITBIH\":\n",
    "\n",
    "    mit_bih_train = pd.read_csv(os.path.join(\"data\",\"ECG\",\"mitbih_train.csv\"),header=None)\n",
    "    target_train = mit_bih_train[LABEL_COL].copy().values\n",
    "    mit_bih_train.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    mit_bih_test = pd.read_csv(os.path.join(\"data\",\"ECG\",\"mitbih_test.csv\"),header=None)\n",
    "    target_test = mit_bih_test[LABEL_COL].copy().values\n",
    "    mit_bih_test.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    mitbih_train = mit_bih_train.values\n",
    "    mitbih_test = mit_bih_test.values\n",
    "\n",
    "    train_features = feature_extraction_window(mitbih_train[:,:,np.newaxis],20,8)\n",
    "    test_features = feature_extraction_window(mitbih_test[:,:,np.newaxis],20,8)\n",
    "\n",
    "elif DATASET == \"PTBDB\":\n",
    "    \n",
    "    abnormal = pd.read_csv(os.path.join(\"data\",\"ECG\",\"ptbdb_abnormal.csv\"),header=None)\n",
    "    normal = pd.read_csv(os.path.join(\"data\",\"ECG\",\"ptbdb_normal.csv\"),header=None)\n",
    "\n",
    "    train_abn, test_abn = train_test_split(abnormal, test_size=0.2, random_state=42)\n",
    "    train_nor, test_nor = train_test_split(normal, test_size=0.2, random_state=42)\n",
    "\n",
    "    ptbdb_train = pd.concat([train_abn, train_nor]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    ptbdb_test = pd.concat([test_abn, test_nor]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    target_train = ptbdb_train[LABEL_COL].copy()\n",
    "    ptbdb_train.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    target_test = ptbdb_test[LABEL_COL].copy()\n",
    "    ptbdb_test.drop(LABEL_COL,axis=1,inplace=True)\n",
    "\n",
    "    ptbdb_train = ptbdb_train.values\n",
    "    ptbdb_test = ptbdb_test.values\n",
    "\n",
    "    train_features = feature_extraction_window(ptbdb_train[:,:,np.newaxis],20,8)\n",
    "    test_features = feature_extraction_window(ptbdb_test[:,:,np.newaxis],20,8)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection & standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecter = VarianceThreshold(2)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecter = selecter.fit(train_features)\n",
    "selected_train = selecter.transform(train_features)\n",
    "selected_test = selecter.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = scaler.fit(selected_train)\n",
    "scaled_train = scaler.transform(selected_train)\n",
    "scaled_test = scaler.transform(selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 878)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xgb\" # [\"svm\", \"random_forest\", \"xgb\"]\n",
    "MODEL = None\n",
    "MODEL_PARAMS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"svm\":\n",
    "    MODEL = SVC()\n",
    "    MODEL_PARAMS = {\n",
    "        \"kernel\":[\"linear\",\"poly\",\"rbf\"],\n",
    "        \"C\":[1.0,5.0,10.0,20.0]\n",
    "    }\n",
    "\n",
    "elif MODEL_NAME == \"random_forest\":\n",
    "    MODEL = RandomForestClassifier()\n",
    "    MODEL_PARAMS = {\n",
    "        \"n_estimators\":range(40,301,20),\n",
    "        \"max_depth\":[3, 5, 8, 12],\n",
    "        \"max_samples\":[0.4, 0.7, 1.0]\n",
    "    }\n",
    "\n",
    "elif MODEL_NAME == \"xgb\":\n",
    "    MODEL = XGBClassifier()\n",
    "    MODEL_PARAMS = {\n",
    "        \"n_estimators\":range(40,101,20),\n",
    "        \"max_depth\":[3, 5, 8],\n",
    "        \"learning_rate\":[1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "\n",
    "else:\n",
    "    print(\"Untested model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(MODEL, MODEL_PARAMS,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(scaled_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = []\n",
    "\n",
    "for i, params in enumerate(pd.DataFrame(clf.cv_results_)[\"params\"]):\n",
    "    if MODEL_NAME == \"svm\":\n",
    "        curr_model = SVC(**params)\n",
    "    elif MODEL_NAME == \"random_forest\":\n",
    "        curr_model = RandomForestClassifier(**params)\n",
    "    elif MODEL_NAME == \"xgb\":\n",
    "        curr_model = XGBClassifier(**params)\n",
    "    \n",
    "    curr_model = curr_model.fit(scaled_train, target_train)\n",
    "\n",
    "    y_true = target_test\n",
    "    y_pred = curr_model.predict(scaled_test)\n",
    "\n",
    "\n",
    "    row = [params]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    row.append(accuracy)\n",
    "    res = list(map(np.mean, precision_recall_fscore_support(y_true, y_pred)))\n",
    "    res2 = list(map(np.std, precision_recall_fscore_support(y_true, y_pred)))\n",
    "    \n",
    "    mean_std = zip(res[:-1], res2[:-1])\n",
    "    for mean_std_tuple in mean_std:\n",
    "        row.extend(list(mean_std_tuple))\n",
    "\n",
    "    results_df.append(row)\n",
    "    print(i)\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Mean precision</th>\n",
       "      <th>Std precision</th>\n",
       "      <th>Mean recall</th>\n",
       "      <th>Std recall</th>\n",
       "      <th>Mean f1</th>\n",
       "      <th>Std f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.001, 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.001, 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.001, 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.001, 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.001, 'max_depth': 5, 'n_es...</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'learning_rate': 0.001, 'max_depth': 5, 'n_es...</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Hyperparameters  Mean precision  \\\n",
       "0  {'learning_rate': 0.001, 'max_depth': 3, 'n_es...           0.503   \n",
       "1  {'learning_rate': 0.001, 'max_depth': 3, 'n_es...           0.503   \n",
       "2  {'learning_rate': 0.001, 'max_depth': 3, 'n_es...           0.515   \n",
       "3  {'learning_rate': 0.001, 'max_depth': 3, 'n_es...           0.512   \n",
       "4  {'learning_rate': 0.001, 'max_depth': 5, 'n_es...           0.543   \n",
       "5  {'learning_rate': 0.001, 'max_depth': 5, 'n_es...           0.532   \n",
       "\n",
       "   Std precision  Mean recall  Std recall  Mean f1  Std f1  \n",
       "0          0.108        0.491       0.061    0.495   0.080  \n",
       "1          0.108        0.491       0.061    0.495   0.080  \n",
       "2          0.114        0.506       0.080    0.510   0.095  \n",
       "3          0.105        0.506       0.080    0.509   0.091  \n",
       "4          0.107        0.522       0.101    0.526   0.093  \n",
       "5          0.094        0.507       0.077    0.513   0.067  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    \"Hyperparameters\",\n",
    "    \"Accuracy\",\n",
    "    \"Mean precision\",\n",
    "    \"Std precision\",\n",
    "    \"Mean recall\",\n",
    "    \"Std recall\",\n",
    "    \"Mean f1\",\n",
    "    \"Std f1\"\n",
    "]\n",
    "\n",
    "pd.DataFrame(results_df,columns=columns).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.87      0.91       810\n",
      "         1.0       0.95      0.98      0.97      2102\n",
      "\n",
      "    accuracy                           0.95      2912\n",
      "   macro avg       0.95      0.92      0.94      2912\n",
      "weighted avg       0.95      0.95      0.95      2912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = classification_report(target_test, clf.predict(scaled_test))\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
